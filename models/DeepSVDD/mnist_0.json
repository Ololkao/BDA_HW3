{"class_name": "Functional", "config": {"name": "model_2", "layers": [{"class_name": "InputLayer", "config": {"batch_input_shape": [null, 2], "dtype": "float32", "sparse": false, "ragged": false, "name": "input_2"}, "name": "input_2", "inbound_nodes": []}, {"class_name": "Dense", "config": {"name": "dense_11", "trainable": true, "dtype": "float32", "units": 500, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "dense_11", "inbound_nodes": [[["input_2", 0, 0, {}]]]}, {"class_name": "Dense", "config": {"name": "dense_12", "trainable": true, "dtype": "float32", "units": 2000, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "dense_12", "inbound_nodes": [[["dense_11", 0, 0, {}]]]}, {"class_name": "Dropout", "config": {"name": "dropout_9", "trainable": true, "dtype": "float32", "rate": 0.2, "noise_shape": null, "seed": null}, "name": "dropout_9", "inbound_nodes": [[["dense_12", 0, 0, {}]]]}, {"class_name": "Dense", "config": {"name": "net_output", "trainable": true, "dtype": "float32", "units": 500, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "net_output", "inbound_nodes": [[["dropout_9", 0, 0, {}]]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.subtract_1", "trainable": true, "dtype": "float32", "function": "math.subtract"}, "name": "tf.math.subtract_1", "inbound_nodes": [["net_output", 0, 0, {"y": [0.0, 0.1822100579738617, 0.13356894254684448, 0.0, 0.2371087372303009, 0.13265228271484375, 0.1126726046204567, 0.10578553378582001, 0.10000000149011612, 0.11548632383346558, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.2061629742383957, 0.2419983148574829, 0.0, 0.10000000149011612, 0.10000000149011612, 0.1437211036682129, 0.0, 0.2350074201822281, 0.0, 0.16797710955142975, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.12220491468906403, 0.13349443674087524, 0.10000000149011612, 0.18643389642238617, 0.25738492608070374, 0.0, 0.3210807144641876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.0, 0.20198313891887665, 0.0, 0.0, 0.0, 0.13854563236236572, 0.0, 0.0, 0.10000000149011612, 0.0, 0.0, 0.17569100856781006, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.28747960925102234, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.17274890840053558, 0.0, 0.2328847348690033, 0.10000000149011612, 0.16351793706417084, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.16584286093711853, 0.23607370257377625, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.3118497431278229, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, 0.0, 0.14807391166687012, 0.15484276413917542, 0.24371321499347687, 0.0, 0.0, 0.0, 0.16193239390850067, 0.0, 0.12515141069889069, 0.3787200152873993, 0.0, 0.10000000149011612, 0.0, 0.4927126467227936, 0.10000000149011612, 0.20977561175823212, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.18459370732307434, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1290488839149475, 0.10423184931278229, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.3516710102558136, 0.2618660032749176, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.14031827449798584, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.0, 0.10000000149011612, 0.11765854805707932, 0.10000000149011612, 0.10803398489952087, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, 0.1274133175611496, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.16745060682296753, 0.0, 0.10000000149011612, 0.25977084040641785, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.34420594573020935, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.3611511290073395, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.18324649333953857, 0.0, 0.30037492513656616, 0.0, 0.2211443930864334, 0.0, 0.0, 0.10679405927658081, 0.16444577276706696, 0.0, 0.0, 0.10000000149011612, 0.1244397908449173, 0.14055684208869934, 0.10000000149011612, 0.10000000149011612, 0.12667585909366608, 0.0, 0.10000000149011612, 0.0, 0.46848931908607483, 0.10000000149011612, 0.10195480287075043, 0.15091942250728607, 0.10000000149011612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10865679383277893, 0.10000000149011612, 0.3064388036727905, 0.10000000149011612, 0.10000000149011612, 0.20151777565479279, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.16751287877559662, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.2607097029685974, 0.0, 0.0, 0.10000000149011612, 0.0, 0.11742236465215683, 0.33419620990753174, 0.10000000149011612, 0.10000000149011612, 0.0, 0.26166796684265137, 0.10000000149011612, 0.11000686138868332, 0.0, 0.2228720337152481, 0.0, 0.2215406745672226, 0.0, 0.0, 0.4332367777824402, 0.10000000149011612, 0.0, 0.38774406909942627, 0.10000000149011612, 0.31566014885902405, 0.1120968759059906, 0.1371326893568039, 0.10000000149011612, 0.11946713924407959, 0.0, 0.0, 0.1241779625415802, 0.1526743471622467, 0.2248769998550415, 0.0, 0.10000000149011612, 0.0, 0.0, 0.1453186720609665, 0.5335686802864075, 0.0, 0.10000000149011612, 0.10000000149011612, 0.30037200450897217, 0.34388700127601624, 0.10000000149011612, 0.19361357390880585, 0.0, 0.0, 0.10000000149011612, 0.0, 0.19375871121883392, 0.0, 0.3074655532836914, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.23529976606369019, 0.3411436378955841, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.2106531262397766, 0.10000000149011612, 0.10000000149011612, 0.25798293948173523, 0.10000000149011612, 0.18524543941020966, 0.0, 0.10000000149011612, 0.10000000149011612, 0.23308998346328735, 0.17293839156627655, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.304779052734375, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.12656454741954803, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.11592888832092285, 0.10000000149011612, 0.11922956258058548, 0.15396156907081604, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.3201153576374054, 0.3181145489215851, 0.289225697517395, 0.11701468378305435, 0.18607459962368011, 0.10000000149011612, 0.0, 0.20890015363693237, 0.0, 0.10000000149011612, 0.26946377754211426, 0.18176136910915375, 0.10000000149011612, 0.10000000149011612, 0.0, 0.3496544659137726, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.11429721117019653, 0.10228732228279114, 0.3348662555217743, 0.0, 0.33599838614463806, 0.10000000149011612, 0.4800882637500763, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.12045922875404358, 0.0, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.22042964398860931, 0.11722081899642944, 0.10000000149011612, 0.18691755831241608, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.36802640557289124, 0.10000000149011612, 0.0, 0.10383427888154984, 0.10000000149011612, 0.5102894902229309, 0.10000000149011612, 0.10000000149011612, 0.20189718902111053, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.16148428618907928, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.17119640111923218, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.2267080545425415, 0.10000000149011612, 0.28081774711608887, 0.16998429596424103, 0.0, 0.10000000149011612, 0.11314278095960617, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.17393699288368225, 0.1682959794998169, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10808165371417999, 0.16021554172039032, 0.26552143692970276, 0.10000000149011612, 0.18673500418663025, 0.10000000149011612, 0.10293328016996384, 0.10000000149011612, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.17007406055927277, 0.12995028495788574, 0.16446924209594727, 0.21180936694145203, 0.2730620503425598, 0.10000000149011612, 0.1686244010925293, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.37896254658699036, 0.0, 0.3638781011104584, 0.14642299711704254, 0.14965341985225677, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.0, 0.1652469038963318, 0.10000000149011612, 0.3294268250465393, 0.2232353836297989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.19940748810768127], "name": null}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.pow_1", "trainable": true, "dtype": "float32", "function": "math.pow"}, "name": "tf.math.pow_1", "inbound_nodes": [["tf.math.subtract_1", 0, 0, {"y": 2, "name": null}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.reduce_sum_1", "trainable": true, "dtype": "float32", "function": "math.reduce_sum"}, "name": "tf.math.reduce_sum_1", "inbound_nodes": [["tf.math.pow_1", 0, 0, {"axis": -1}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.reduce_mean_1", "trainable": true, "dtype": "float32", "function": "math.reduce_mean"}, "name": "tf.math.reduce_mean_1", "inbound_nodes": [["tf.math.reduce_sum_1", 0, 0, {}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.__operators__.add_1", "trainable": true, "dtype": "float32", "function": "__operators__.add"}, "name": "tf.__operators__.add_1", "inbound_nodes": [["tf.math.reduce_mean_1", 0, 0, {"y": 5.858125615119934e-05, "name": null}]]}, {"class_name": "AddLoss", "config": {"name": "add_loss_1", "trainable": true, "dtype": "float32", "unconditional": false}, "name": "add_loss_1", "inbound_nodes": [[["tf.__operators__.add_1", 0, 0, {}]]]}], "input_layers": [["input_2", 0, 0]], "output_layers": [["tf.math.reduce_sum_1", 0, 0]]}, "keras_version": "2.6.0", "backend": "tensorflow"}