{"class_name": "Functional", "config": {"name": "model_20", "layers": [{"class_name": "InputLayer", "config": {"batch_input_shape": [null, 2], "dtype": "float32", "sparse": false, "ragged": false, "name": "input_14"}, "name": "input_14", "inbound_nodes": []}, {"class_name": "Dense", "config": {"name": "dense_89", "trainable": true, "dtype": "float32", "units": 830, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "dense_89", "inbound_nodes": [[["input_14", 0, 0, {}]]]}, {"class_name": "Dense", "config": {"name": "dense_90", "trainable": true, "dtype": "float32", "units": 3320, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "dense_90", "inbound_nodes": [[["dense_89", 0, 0, {}]]]}, {"class_name": "Dropout", "config": {"name": "dropout_69", "trainable": true, "dtype": "float32", "rate": 0.2, "noise_shape": null, "seed": null}, "name": "dropout_69", "inbound_nodes": [[["dense_90", 0, 0, {}]]]}, {"class_name": "Dense", "config": {"name": "net_output", "trainable": true, "dtype": "float32", "units": 830, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": {"class_name": "L2", "config": {"l2": 0.10000000149011612}}, "kernel_constraint": null, "bias_constraint": null}, "name": "net_output", "inbound_nodes": [[["dropout_69", 0, 0, {}]]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.subtract_13", "trainable": true, "dtype": "float32", "function": "math.subtract"}, "name": "tf.math.subtract_13", "inbound_nodes": [["net_output", 0, 0, {"y": [0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.22305113077163696, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.15266990661621094, 0.10000000149011612, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.15916579961776733, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1778784841299057, 0.10000000149011612, 0.2290908247232437, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.20189949870109558, 0.10000000149011612, 0.10000000149011612, 0.3691757321357727, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.2820550799369812, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10172414034605026, 0.2002069056034088, 0.1885288655757904, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.22538648545742035, 0.10000000149011612, 0.1664508432149887, 0.17440791428089142, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.23886729776859283, 0.10000000149011612, 0.13690359890460968, 0.10000000149011612, 0.11053529381752014, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.18010328710079193, 0.10000000149011612, 0.10055096447467804, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.15492072701454163, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.15297986567020416, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.12371962517499924, 0.10000000149011612, 0.1387290209531784, 0.10000000149011612, 0.10000000149011612, 0.0, 0.125152125954628, 0.10000000149011612, 0.0, 0.10775765776634216, 0.10000000149011612, 0.1553671658039093, 0.11813361942768097, 0.11539735645055771, 0.0, 0.1346716284751892, 0.10000000149011612, 0.12770113348960876, 0.10000000149011612, 0.10000000149011612, 0.0, 0.12524937093257904, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10748853534460068, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.14463478326797485, 0.10000000149011612, 0.31023192405700684, 0.10000000149011612, 0.10000000149011612, 0.0, 0.1878996193408966, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1823742389678955, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1958794742822647, 0.16667024791240692, 0.20759399235248566, 0.10000000149011612, 0.10000000149011612, 0.14182545244693756, 0.19952204823493958, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1459038108587265, 0.13807043433189392, 0.10000000149011612, 0.17156679928302765, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.22656072676181793, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.28321900963783264, 0.0, 0.14305910468101501, 0.10000000149011612, 0.10000000149011612, 0.1659303903579712, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.15443742275238037, 0.10000000149011612, 0.10000000149011612, 0.10729324072599411, 0.33899447321891785, 0.10633865743875504, 0.10000000149011612, 0.10000000149011612, 0.10776428133249283, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.14967381954193115, 0.1790601760149002, 0.0, 0.10000000149011612, 0.10000000149011612, 0.13780535757541656, 0.0, 0.10000000149011612, 0.10000000149011612, 0.4358648359775543, 0.12106510996818542, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.16380080580711365, 0.10000000149011612, 0.12508635222911835, 0.10000000149011612, 0.0, 0.10000000149011612, 0.17427103221416473, 0.10000000149011612, 0.29220712184906006, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.2487454116344452, 0.28531602025032043, 0.10000000149011612, 0.10000000149011612, 0.177742600440979, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10978958755731583, 0.10000000149011612, 0.0, 0.10000000149011612, 0.1479293704032898, 0.1759634017944336, 0.1942090392112732, 0.247615247964859, 0.16736692190170288, 0.23352329432964325, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10498037189245224, 0.10000000149011612, 0.24960877001285553, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.13194996118545532, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.19236642122268677, 0.0, 0.0, 0.21518288552761078, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.15981097519397736, 0.0, 0.0, 0.0, 0.12743787467479706, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.11898619681596756, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.14481940865516663, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.13074876368045807, 0.10000000149011612, 0.1931026577949524, 0.10000000149011612, 0.12784388661384583, 0.10000000149011612, 0.10066279023885727, 0.10000000149011612, 0.10000000149011612, 0.23749572038650513, 0.0, 0.10000000149011612, 0.15532860159873962, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.1967041939496994, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.15511590242385864, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.2916962802410126, 0.0, 0.10000000149011612, 0.20305144786834717, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.25850576162338257, 0.18181096017360687, 0.12539848685264587, 0.28931522369384766, 0.1364409625530243, 0.10000000149011612, 0.10694343596696854, 0.10000000149011612, 0.14720726013183594, 0.2868688702583313, 0.10185476392507553, 0.10000000149011612, 0.10000000149011612, 0.32836416363716125, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.16511040925979614, 0.10000000149011612, 0.1378324180841446, 0.10000000149011612, 0.11942686885595322, 0.14925311505794525, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.15251506865024567, 0.10000000149011612, 0.10000000149011612, 0.17646870017051697, 0.122965008020401, 0.17090147733688354, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.12521278858184814, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.14817851781845093, 0.0, 0.10000000149011612, 0.13897563517093658, 0.20670728385448456, 0.0, 0.10000000149011612, 0.2225000411272049, 0.1812879741191864, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.21051444113254547, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.14586922526359558, 0.2865370213985443, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.18868260085582733, 0.11618959903717041, 0.0, 0.14708897471427917, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.11008038371801376, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.19868265092372894, 0.10000000149011612, 0.11045163869857788, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.12396709620952606, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.23942367732524872, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.15101760625839233, 0.14384345710277557, 0.12874557077884674, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.2803327441215515, 0.0, 0.0, 0.0, 0.12622500956058502, 0.10000000149011612, 0.0, 0.23121419548988342, 0.1006646379828453, 0.15688888728618622, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.158359095454216, 0.10000000149011612, 0.0, 0.10000000149011612, 0.11334552615880966, 0.0, 0.11258520185947418, 0.10000000149011612, 0.10000000149011612, 0.0, 0.14096565544605255, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.17231161892414093, 0.0, 0.10000000149011612, 0.10000000149011612, 0.2154730260372162, 0.21480584144592285, 0.10000000149011612, 0.0, 0.27849799394607544, 0.2177044153213501, 0.0, 0.24432751536369324, 0.10000000149011612, 0.10000000149011612, 0.25333675742149353, 0.10000000149011612, 0.0, 0.0, 0.2659539580345154, 0.17167745530605316, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.17588114738464355, 0.18520498275756836, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1496431976556778, 0.0, 0.11729153245687485, 0.14099179208278656, 0.10000000149011612, 0.10000000149011612, 0.23220635950565338, 0.10000000149011612, 0.16752193868160248, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.12763988971710205, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.13499052822589874, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.15799866616725922, 0.10000000149011612, 0.10000000149011612, 0.14814971387386322, 0.1544165164232254, 0.10000000149011612, 0.4508354365825653, 0.10000000149011612, 0.14267116785049438, 0.10000000149011612, 0.15557265281677246, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.26408395171165466, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.18820753693580627, 0.10000000149011612, 0.10000000149011612, 0.0, 0.3474960923194885, 0.10000000149011612, 0.16595879197120667, 0.0, 0.14487764239311218, 0.32976776361465454, 0.10517754405736923, 0.10000000149011612, 0.22324205935001373, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.13349390029907227, 0.10000000149011612, 0.3323802351951599, 0.10421355068683624, 0.10000000149011612, 0.10000000149011612, 0.0, 0.0, 0.10000000149011612, 0.1762075573205948, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1317550390958786, 0.10085249692201614, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10956219583749771, 0.0, 0.10000000149011612, 0.12561705708503723, 0.0, 0.10000000149011612, 0.0, 0.23168526589870453, 0.10000000149011612, 0.10000000149011612, 0.1898031383752823, 0.10000000149011612, 0.10000000149011612, 0.0, 0.19451719522476196, 0.12515458464622498, 0.10000000149011612, 0.13903476297855377, 0.0, 0.10000000149011612, 0.0, 0.10000000149011612, 0.15616197884082794, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.26356735825538635, 0.10000000149011612, 0.15883351862430573, 0.14947134256362915, 0.10000000149011612, 0.269968718290329, 0.10000000149011612, 0.12045028060674667, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10680440068244934, 0.0, 0.10000000149011612, 0.12774285674095154, 0.10000000149011612, 0.10000000149011612, 0.19279195368289948, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.1327381283044815, 0.10000000149011612, 0.10000000149011612, 0.13015025854110718, 0.1420549750328064, 0.0, 0.10000000149011612, 0.11802113056182861, 0.10000000149011612, 0.0, 0.3793042004108429, 0.0, 0.10000000149011612, 0.10000000149011612, 0.12000654637813568, 0.21543827652931213, 0.10000000149011612, 0.13627879321575165, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.0, 0.11005376279354095, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.16382059454917908, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10418760776519775, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10462909936904907, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.13974973559379578, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10107900202274323, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10235175490379333, 0.10000000149011612, 0.0, 0.0, 0.0, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.0, 0.10000000149011612, 0.0, 0.125563383102417], "name": null}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.pow_13", "trainable": true, "dtype": "float32", "function": "math.pow"}, "name": "tf.math.pow_13", "inbound_nodes": [["tf.math.subtract_13", 0, 0, {"y": 2, "name": null}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.reduce_sum_13", "trainable": true, "dtype": "float32", "function": "math.reduce_sum"}, "name": "tf.math.reduce_sum_13", "inbound_nodes": [["tf.math.pow_13", 0, 0, {"axis": -1}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.math.reduce_mean_13", "trainable": true, "dtype": "float32", "function": "math.reduce_mean"}, "name": "tf.math.reduce_mean_13", "inbound_nodes": [["tf.math.reduce_sum_13", 0, 0, {}]]}, {"class_name": "TFOpLambda", "config": {"name": "tf.__operators__.add_13", "trainable": true, "dtype": "float32", "function": "__operators__.add"}, "name": "tf.__operators__.add_13", "inbound_nodes": [["tf.math.reduce_mean_13", 0, 0, {"y": 7.484910881519317e-05, "name": null}]]}, {"class_name": "AddLoss", "config": {"name": "add_loss_13", "trainable": true, "dtype": "float32", "unconditional": false}, "name": "add_loss_13", "inbound_nodes": [[["tf.__operators__.add_13", 0, 0, {}]]]}], "input_layers": [["input_14", 0, 0]], "output_layers": [["tf.math.reduce_sum_13", 0, 0]]}, "keras_version": "2.6.0", "backend": "tensorflow"}